\section{Model specifications}
\label{chapter4_section2}


There are 9 base models considered in the projet: 3 nowcasting models (dynamic factor model, MIDAS regression and mixed frequency Bayesian VAR), 3 econometrics models (VAR, Bayesian VAR and time-varying Bayesian VAR), and 3 machine learning models (LSTM, random forest and gradient boosting).

The dynamic factor model is trained as the monthly frequency. It includes the full macroeconomic dataset (31 monthly series), and one quarterly series of GDP. It is fundamentally determined by two parameters: the number of dynamic factors $p$ included in the model, and the number of common shocks $q$ assumed to drive the factor dynamics. The optimal specification is determined by a grid search, using the last 25\% of the sample as a test set. This yields $p = 4$ and $q = 2$.

The MIDAS regression is trained on quarterly frequency, even though it includes also monthly regressors. The small dataset is used for monthly series, to keep the approach parsimonious. Following the MIDAS litterature on quarterly/monthly MIDAS models (see e.g. \cite{Ghysels2016}), the number of high frequency lags is set to match the number of lagged periods of the lower frequency variable, and keep the specification parsimonious. This leads to set $p=2$ low frequency lags and $q=6$ high frequency lags.

The mixed frequency Bayesian VAR is trained at the monthly frequency. Because it adopts the Bayesian approach, the priors for the VAR coefficients $\beta$ and the residual covariance matrix $\Sigma$ must be specified. The specification retained is the same as the regular Bayesian VAR and is described below.

The standard VAR model has to be trained on a balanced panel. Due to the quarterly frequency of the GDP growth series, the whole model must be trained on quarterly data. Also, because VAR models perform poorly on very large numbers of feature, the small dataset is used. The only remaining parameter to specify is the number of lags. Following standard practices, it is determined as the one that minimizes the Akaike Information Criterion (AIC). This yields $p=2$.

The Bayesian VAR is similar to the regular VAR, but benefits from the Minnesota prior. The hyperparameter values have then to be specified. The litterature provides strandard values (\cite{Litterman1986}), but it is better to optimize the values. This is done on a grid search seeking to optimize the marginal likelihood $f(y)$ of the model. This results in $\rho = 0.85$, $\lambda_1 = 0.4$, $\lambda_2 = 0.3$, $\lambda_3 = 1.5$ and $\lambda_4 = 1000$.
 
The time-varying Bayesian VAR also follows the same specification as the VAR and BVAR models., but its dynamic parameters involve specific hyperparameters. The values for the dynamic equations are $\rho_i = \gamma_i = \alpha_i = 0.9$. The equilibrium values $b_i, s_i$ and $d_i$ are derived from static OLS estimates. For the inverse Wishart priors on the variance-covariance hyperparameters $\Omega_i$ and $\Psi_i$, the degrees of freedom are set to a small value of 5 additional to the parameter dimension. The scale parameters are set to $\Upsilon_0 = \Theta_0 = 0.001 I$. Similarly, the shape and scale parameters of the inverse Gamma prior distribution on $\phi_i$ are set to $\kappa_0 = 5$ and $\omega_0 = 0.01$. These priors are mildly informative, being sufficiently loose to allow for a signicant degree of time variation in the dynamic parameters, but sufficiently restrictive to avoid implausible behaviours. Finally, the initial period variance scaling terms are set to $\tau = \mu = \eps = 5$ in order to obtain a variance over the initial periods which is roughly equivalent to that prevailing for the rest of the sample.

The LSTM model is trained on the small dataset at the quarterly frequency. To make it similar to the VAR models, the output vector $y_t$ is chosen to be the set of the 9 features of the small dataset at period $t$, while the input features $x_t$ is the first two lags of the output values. That is, the model is of the form $y_t = f(x_t) = f(y_{t-1}, y_{t-2})$. Predictions for $t+h$ can then be obtained easily in a recursive way by predicting $y_{t+1}, \cdots, y_{t+h}$. The optimal number of layers and cells is estimated on a test sample representing 20\% of the sample. One hidden layer with only 3 cells is found to be optimal, resulting in a model in an encoder-like fashion. The model is trained on 500 epochs, with a 25\% dropout rate on the hidden layer to prevent overfitting, and an ADAM optimizer.

The random forest algorithm is designed to work well on many features. For this reason, it uses the full dataset of 31 features. The setting is divided in two different models. The first model predicts GDP from its own lagged values and lags of the monthly regressors. In essence, it is thus similar to the MIDAS regression, except that the regression is estimated by random forest on the features rather than using the standard MIDAS approach. Consistently with the MIDAS model, it includes 2 lags of GDP and 6 lags of the monthly features. The second model aims at predicting all the non-GDP monthly features. It is designed as a large VAR model: one random forest model is estimated for each feature, the regressors being lags of the whole set of monthly features. This gives a system of random forest models which once put together act like a large VAR. Random forests work best with weak learners, so given the number of features, a maximum depth of 5 is granted, for a model with 100 trees.

\newpage

The gradient boosting model, finally, is build to act a "boosted VAR". It uses the small dataset, on a quarterly frequency. Each feature in the dataset is regressed by gradient boosting on 2 of its own lags and other feature lags. The resulting system can be interpreted as a VAR with 2 lags, but with a boosting, nonlinear flavour. There is a trade-off between the learning rate $\alpha$ and the number of trees $M$. A reasonable choice to carry enough flexibility while avoiding overfitting consists in setting $\alpha=0.1$ and $M=100$. Each tree is made a weak learner by limitting the depth to 5.

This constitutes the range of base models. To these 9 models, a number of benchmarks or naive models can be added for the sake of comparison. The first is simply a static predictive model using the last known sample value before prediction. The second is a maximum likelihood VAR model using Ridge regularization. The third  consists in the GDP predictions produced by Nowcasting.com. Finally, the last benchmark is the GDP predictions produced by Bloomberg\footnote{The author is grateful to Adam Majewski for providing the Bloomberg series of predictions.} two months before release.



