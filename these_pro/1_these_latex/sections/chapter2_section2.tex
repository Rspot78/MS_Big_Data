\section{Project design}
\label{chapter2_section2}

The starting point of the project only consisted in replicating the dynamic factor model used by Now-casting.com for the United States. Quickly however, the idea emerged that the project could be more comprehensive and cover the wider question of nowcasting in general. Indeed, the prediction of key economic variables such as GDP at short term or very short term represents a major challenge for the finance industry. This problem, formally known as nowcasting (a contraction of ``now'' and ``forecasting'') has attracted the attention of economists and econometrician for more or less two decades. The data science industry on the other hand has remained more agnostic regarding this issue, and has simply applied regular machine learning methods to obtain short-term predictions. Today, it is still unclear what is the best approach to predict macroeconomic variables in the short term. The question is rendered more difficult by the lack of exchange between the different fields, machine learning models being often unknown to econometrician, and vice versa, data scientists often having a limited culture in econometric models.

The approach of the project would thus consist in testing a range of different methodologies from different fields, in an attempt to assess which one appears most suitable for the purpose of nowcasts. Concretely, three fields were considered as candidates: nowcasting models, econometrics, and machine learning.

Nowcasting models are statistical models that are specifically tailored for short term predictions. In particular, these models account for two specificities of nowcasting models: mixed frequencies, and missing values. Indeed, nowcasting typically involves the prediction of some low frequency variable (for instance quarterly GDP) from the information of higher frequency features (for instance monthly data like industrial production and employment) to obtain insight on the yet unknown quarterly realization. Because high frequency variables are usually published asynchronously (for instance, not all monthly economic variables are updated the same day in the course of a given month), panels are rarely balanced. Nowcasting models are thus built to cope with these two aspects. Three models were retained in this field for the project: the dynamic factor model of \cite{Giannone2008}; the MIDAS regression of \cite{Ghysels2004}; and the mixed frequency Bayesian VAR of \cite{Schorfheide2015}.

Econometrics has a long tradition of prediction models. Since the seminal contribution of \cite{Sims1980}, Vector Autoregressions (VAR models, in short) have become the workhorse of macroeconomics. The methodology has been widely adopted by Central Banks and financial institutions for their prediction routines. VAR models are appealing because they are flexible and integrate multivariate settings. They also achieve good predictive performance. Three VAR models are considered for the project: a simple maximum likelihood VAR; a Bayesian VAR in the line of \cite{Karlsson2012}; and a time-varying VAR, following the methodology of \cite{Primiceri2005}.

Finally, Machine learning models have become central in data analysis with the rise of big data technologies in the 2010's. Powerful models can now be trained and used for prediction purposes thanks to the power of computers and distributed algorithms. Three machine learning methodologies are used in this project: the LSTM model of \cite{Hochreiter1997}, since it is conceptually closest to the VAR model from the econometrics field; the random forest model of \cite{Breiman2001}; and the gradient boosting approach of \cite{Friedman2001}.



