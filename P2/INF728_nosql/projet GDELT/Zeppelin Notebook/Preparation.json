{"paragraphs":[{"text":"%md\n# Projet GDELT - Script ETL - janvier 2020\n#### Auteur : \n\n### *Partie 2 : Préparation des données pour chaque problématique et sauvegarde dans Cassandra*\n\n- Afficher le nombre d’articles/évènements qu’il y a eu pour chaque triplet (jour, pays de l’évènement, langue de l’article).\n\n- Pour un pays donné en paramètre, afficher les évènements qui y ont eu lieu triés par le nombre de mentions (tri décroissant); permetter une agrégation par jour/mois/année\n\n- Pour une source de donnés passée en paramètre (gkg.SourceCommonName), afficher les thèmes, personnes, lieux dont les articles de cette sources parlent, ainsi que le nombre d’articles et le ton moyen des articles (pour chaque thème/personne/lieu); permetter une agrégation par jour/mois/année.\n\n- Dresser la cartographie des relations entre les pays d’après le ton des articles : pour chaque paire (pays1, pays2), calculer le nombre d’article, le ton moyen (aggrégations sur Année/Mois/Jour, filtrage par pays ou carré de coordonnées)\n","dateUpdated":"2020-01-28T15:00:40+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Projet GDELT - Script ETL - janvier 2020</h1>\n<h4>Auteur : </h4>\n<h3><em>Partie 2 : Préparation des données pour chaque problématique et sauvegarde dans Cassandra</em></h3>\n<ul>\n  <li>\n  <p>Afficher le nombre d’articles/évènements qu’il y a eu pour chaque triplet (jour, pays de l’évènement, langue de l’article).</p></li>\n  <li>\n  <p>Pour un pays donné en paramètre, afficher les évènements qui y ont eu lieu triés par le nombre de mentions (tri décroissant); permetter une agrégation par jour/mois/année</p></li>\n  <li>\n  <p>Pour une source de donnés passée en paramètre (gkg.SourceCommonName), afficher les thèmes, personnes, lieux dont les articles de cette sources parlent, ainsi que le nombre d’articles et le ton moyen des articles (pour chaque thème/personne/lieu); permetter une agrégation par jour/mois/année.</p></li>\n  <li>\n  <p>Dresser la cartographie des relations entre les pays d’après le ton des articles : pour chaque paire (pays1, pays2), calculer le nombre d’article, le ton moyen (aggrégations sur Année/Mois/Jour, filtrage par pays ou carré de coordonnées)</p></li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1580223640658_-1054383219","id":"20200122-211153_886918093","dateCreated":"2020-01-28T15:00:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:344"},{"text":"import sys.process._\nimport java.net.URL\nimport java.io.File\nimport java.io.File\nimport java.nio.file.{Files, StandardCopyOption}\nimport java.net.HttpURLConnection \nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.input.PortableDataStream\nimport java.util.zip.ZipInputStream\nimport java.io.BufferedReader\nimport java.io.InputStreamReader\nimport com.amazonaws.services.s3.AmazonS3Client\nimport com.amazonaws.auth.BasicAWSCredentials\nimport com.datastax.spark.connector.cql.CassandraConnector\nimport org.apache.spark.sql.cassandra._\n","dateUpdated":"2020-01-28T15:00:40+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import sys.process._\nimport java.net.URL\nimport java.io.File\nimport java.io.File\nimport java.nio.file.{Files, StandardCopyOption}\nimport java.net.HttpURLConnection\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.input.PortableDataStream\nimport java.util.zip.ZipInputStream\nimport java.io.BufferedReader\nimport java.io.InputStreamReader\nimport com.amazonaws.services.s3.AmazonS3Client\nimport com.amazonaws.auth.BasicAWSCredentials\nimport com.datastax.spark.connector.cql.CassandraConnector\nimport org.apache.spark.sql.cassandra._\n"}]},"apps":[],"jobName":"paragraph_1580223640659_-1054767968","id":"20200122-220213_1491631344","dateCreated":"2020-01-28T15:00:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:345"},{"text":"// Configuration de l'accès au bucket S3\n    \nval AWS_ID = \"XXX\"\nval AWS_KEY = \"XXX\"\n// la classe AmazonS3Client n'est pas serializable\n// on rajoute l'annotation @transient pour dire a Spark de ne pas essayer de serialiser cette classe et l'envoyer aux executeurs\n@transient val awsClient = new AmazonS3Client(new BasicAWSCredentials(AWS_ID, AWS_KEY))\n\nsc.hadoopConfiguration.set(\"fs.s3a.access.key\", AWS_ID) // mettre votre ID du fichier credentials.csv\nsc.hadoopConfiguration.set(\"fs.s3a.secret.key\", AWS_KEY) // mettre votre secret du fichier credentials.csv","dateUpdated":"2020-01-28T15:01:20+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"warning: there was one deprecation warning; re-run with -deprecation for details\nAWS_ID: String = AKIA2X2E6N2TFHVA7BZF\nAWS_KEY: String = AGhzF2xRyNf0M0pbnf9wIP24XBmYl5DyR6293t41\nawsClient: com.amazonaws.services.s3.AmazonS3Client = com.amazonaws.services.s3.AmazonS3Client@19c41809\n"}]},"apps":[],"jobName":"paragraph_1580223640659_-1054767968","id":"20200123-075523_1991036403","dateCreated":"2020-01-28T15:00:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:346"},{"text":"// Décompression des fichiers et transformation en Data Frame\n\n// EVENTS\n// Jeu de données events en anglais\nval events_english_RDD = sc.binaryFiles(\"s3a://bucket-gdelt2019/201901[0-9]*.export.CSV.zip\").\n   flatMap {  // decompresser les fichiers\n       case (name: String, content: PortableDataStream) =>\n          val zis = new ZipInputStream(content.open)\n          Stream.continually(zis.getNextEntry).\n                takeWhile{case null => zis.close(); false\n                       case _ => true}.\n                flatMap { _ =>\n                    val br = new BufferedReader(new InputStreamReader(zis))\n                    Stream.continually(br.readLine()).takeWhile(_ != null)\n                }\n    }\nval events_english_DF = events_english_RDD.map(x => x.split(\"\\t\")).map(row => row.mkString(\";\")).map(x => x.split(\";\")).toDF()\n\n// Jeu de données events autres langues\nval events_others_RDD = sc.binaryFiles(\"s3a://bucket-gdelt2019/201901[0-9]*.translation.export.CSV.zip\").\n   flatMap {  // decompresser les fichiers\n       case (name: String, content: PortableDataStream) =>\n          val zis = new ZipInputStream(content.open)\n          Stream.continually(zis.getNextEntry).\n                takeWhile{case null => zis.close(); false\n                       case _ => true}.\n                flatMap { _ =>\n                    val br = new BufferedReader(new InputStreamReader(zis))\n                    Stream.continually(br.readLine()).takeWhile(_ != null)\n                }\n    }\nval events_others_DF = events_others_RDD.map(x => x.split(\"\\t\")).map(row => row.mkString(\";\")).map(x => x.split(\";\")).toDF()\n\n// GKG\n// Jeu de données du graphe des relations en anglais\nval gkg_english_RDD = sc.binaryFiles(\"s3a://bucket-gdelt2019/201901*.gkg.csv.zip\").\n   flatMap {  // decompresser les fichiers\n       case (name: String, content: PortableDataStream) =>\n          val zis = new ZipInputStream(content.open)\n          Stream.continually(zis.getNextEntry).\n                takeWhile{case null => zis.close(); false\n                       case _ => true}.\n                flatMap { _ =>\n                    val br = new BufferedReader(new InputStreamReader(zis))\n                    Stream.continually(br.readLine()).takeWhile(_ != null)\n                }\n    }\n    \n// Jeu de données du graphe des relations autres langues\nval gkg_other_RDD = sc.binaryFiles(\"s3a://bucket-gdelt2019/201901*.translation.gkg.csv.zip\").\n   flatMap {  // decompresser les fichiers\n       case (name: String, content: PortableDataStream) =>\n          val zis = new ZipInputStream(content.open)\n          Stream.continually(zis.getNextEntry).\n                takeWhile{case null => zis.close(); false\n                       case _ => true}.\n                flatMap { _ =>\n                    val br = new BufferedReader(new InputStreamReader(zis))\n                    Stream.continually(br.readLine()).takeWhile(_ != null)\n                }\n    }\n\n// MENTIONS\n// Jeu de données mention en anglais\nval mentions_english_RDD = sc.binaryFiles(\"s3a://bucket-gdelt2019/201901*.mentions.CSV.zip\").\n   flatMap {  // decompresser les fichiers\n       case (name: String, content: PortableDataStream) =>\n          val zis = new ZipInputStream(content.open)\n          Stream.continually(zis.getNextEntry).\n                takeWhile{case null => zis.close(); false\n                       case _ => true}.\n                flatMap { _ =>\n                    val br = new BufferedReader(new InputStreamReader(zis))\n                    Stream.continually(br.readLine()).takeWhile(_ != null)\n                }\n    }\nval mentions_english_DF = mentions_english_RDD.map(x => x.split(\"\\t\")).map(row => row.mkString(\";\")).map(x => x.split(\";\")).toDF()\n\n// Jeu de données mention autres langues\nval mentions_others_RDD = sc.binaryFiles(\"s3a://bucket-gdelt2019/201901[0-9]*.translation.mentions.CSV.zip\").\n   flatMap {  // decompresser les fichiers\n       case (name: String, content: PortableDataStream) =>\n          val zis = new ZipInputStream(content.open)\n          Stream.continually(zis.getNextEntry).\n                takeWhile{case null => zis.close(); false\n                       case _ => true}.\n                flatMap { _ =>\n                    val br = new BufferedReader(new InputStreamReader(zis))\n                    Stream.continually(br.readLine()).takeWhile(_ != null)\n                }\n    }\nval mentions_others_DF = mentions_others_RDD.map(x => x.split(\"\\t\")).map(row => row.mkString(\";\")).map(x => x.split(\";\")).toDF()","dateUpdated":"2020-01-28T15:00:40+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"events_english_RDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[366] at flatMap at <console>:104\nevents_english_DF: org.apache.spark.sql.DataFrame = [value: array<string>]\nevents_others_RDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[371] at flatMap at <console>:119\nevents_others_DF: org.apache.spark.sql.DataFrame = [value: array<string>]\ngkg_english_RDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[376] at flatMap at <console>:135\ngkg_other_RDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[378] at flatMap at <console>:149\nmentions_english_RDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[380] at flatMap at <console>:164\nmentions_english_DF: org.apache.spark.sql.DataFrame = [value: array<string>]\nmentions_others_RDD: org.apache.spark.rdd.RDD[String]..."}]},"apps":[],"jobName":"paragraph_1580223640660_-1056691713","id":"20200122-211339_844922569","dateCreated":"2020-01-28T15:00:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:347"},{"text":"// Union des datasets en anglais et autres langues\nval events_DF = events_english_DF.union(events_others_DF)\n//val gkg_DF = gkg_english_DF.union(gkg_others_DF)\nval mentions_DF = mentions_english_DF.union(mentions_others_DF)","dateUpdated":"2020-01-28T15:00:40+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"events_DF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [value: array<string>]\nmentions_DF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [value: array<string>]\n"}]},"apps":[],"jobName":"paragraph_1580223640660_-1056691713","id":"20200122-211349_1356251194","dateCreated":"2020-01-28T15:00:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:348"},{"text":"%md\n**REQUETE 1&2**\nAfficher le nombre d’articles/évènements qu’il y a eu pour chaque triplet (jour, pays de l’évènement, langue de l’article)\nPour un pays donné en paramètre, afficher les évènements qui y ont eu lieu triés par le nombre de mentions (tri décroissant); permetter une agrégation par jour/mois/année","dateUpdated":"2020-01-28T15:00:40+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p><strong>REQUETE 1&amp;2</strong><br/>Afficher le nombre d’articles/évènements qu’il y a eu pour chaque triplet (jour, pays de l’évènement, langue de l’article)<br/>Pour un pays donné en paramètre, afficher les évènements qui y ont eu lieu triés par le nombre de mentions (tri décroissant); permetter une agrégation par jour/mois/année</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1580223640661_-1057076462","id":"20200122-211435_281455077","dateCreated":"2020-01-28T15:00:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:349"},{"text":"// On récupère les champs de la table events qui nous intéressent pour cette requête\nval events = events_DF.withColumn(\"_tmp\", $\"value\").select(\n    $\"_tmp\".getItem(0).as(\"globaleventid\"),\n    $\"_tmp\".getItem(1).as(\"day_event\"),\n    $\"_tmp\".getItem(53).as(\"actioncountry\")\n    )\n    \n// On récupère les champs de la table mentions qui nous intéressent pour cette requête\nval mentions = mentions_DF.withColumn(\"_tmp\", $\"value\").select(\n    $\"_tmp\".getItem(0).as(\"globaleventid\"),\n    $\"_tmp\".getItem(14).as(\"language\"),\n    $\"_tmp\".getItem(5).as(\"article\"),\n    $\"_tmp\".getItem(2).as(\"day_mention\")\n    )","dateUpdated":"2020-01-28T15:00:40+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"events: org.apache.spark.sql.DataFrame = [globaleventid: string, day_event: string ... 1 more field]\nmentions: org.apache.spark.sql.DataFrame = [globaleventid: string, language: string ... 2 more fields]\n"}]},"apps":[],"jobName":"paragraph_1580223640661_-1057076462","id":"20200122-211452_2022296385","dateCreated":"2020-01-28T15:00:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:350"},{"text":"// Preprocessing des données\n// On traite les données du dataframe mentions\nval mentions_trans =  mentions.withColumn(\"language\",\n    when(col(\"language\").isNull, \"eng\").\n    otherwise(substring_index(col(\"language\"), \":\", -1))\n    ).withColumn(\"day_mention\", substring(col(\"day_mention\"), 1, 8))\n    \nval mentions_cast = mentions_trans.withColumn(\"globaleventid\", mentions_trans(\"globaleventid\").cast(\"Int\")).withColumn(\"day_mention\", mentions_trans(\"day_mention\").cast(\"Int\"))\n    \n// On traite les données du dataframe events\nval events_cast = events.withColumn(\"day_event\", events(\"day_event\").cast(\"Int\")).withColumn(\"globaleventid\", events(\"globaleventid\").cast(\"Int\"))","dateUpdated":"2020-01-28T15:00:40+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"mentions_trans: org.apache.spark.sql.DataFrame = [globaleventid: string, language: string ... 2 more fields]\nmentions_cast: org.apache.spark.sql.DataFrame = [globaleventid: int, language: string ... 2 more fields]\nevents_cast: org.apache.spark.sql.DataFrame = [globaleventid: int, day_event: int ... 1 more field]\n"}]},"apps":[],"jobName":"paragraph_1580223640661_-1057076462","id":"20200122-211510_1936910672","dateCreated":"2020-01-28T15:00:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:351"},{"text":"// DATA REQUETE 1\n// On crée 1 table agrégée pour compter le nombre d'articles par évènement\nval mentions_gby = mentions_cast.groupBy(\"day_mention\",\"language\",\"globaleventid\").agg(countDistinct($\"article\").as(\"nombre_articles\"))\nval requete1_df = mentions_gby.join(events_cast, \"globaleventid\")","dateUpdated":"2020-01-28T15:00:40+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"mentions_gby: org.apache.spark.sql.DataFrame = [day_mention: int, language: string ... 2 more fields]\nrequete1_df: org.apache.spark.sql.DataFrame = [globaleventid: int, day_mention: int ... 4 more fields]\n"}]},"apps":[],"jobName":"paragraph_1580223640662_-1055922215","id":"20200122-211543_205777777","dateCreated":"2020-01-28T15:00:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:352"},{"text":"//requete1_df.show()","dateUpdated":"2020-01-28T15:00:40+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1580223640662_-1055922215","id":"20200123-080514_1436574915","dateCreated":"2020-01-28T15:00:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:353"},{"text":"//DATA REQUETE 2\nval mentions_cnt  = mentions_cast.groupBy(\"globaleventid\").agg(count($\"globaleventid\").as(\"nombre_mentions\"))\nval requete2_df  = events_cast.join(mentions_cnt, \"globaleventid\")","dateUpdated":"2020-01-28T15:00:40+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"mentions_cnt: org.apache.spark.sql.DataFrame = [globaleventid: int, nombre_mentions: bigint]\nrequete2_df: org.apache.spark.sql.DataFrame = [globaleventid: int, day_event: int ... 2 more fields]\n"}]},"apps":[],"jobName":"paragraph_1580223640663_-1056306964","id":"20200122-211559_516850858","dateCreated":"2020-01-28T15:00:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:354"},{"text":"//requete2_df.show()","dateUpdated":"2020-01-28T15:00:40+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1580223640663_-1056306964","id":"20200123-143447_126416826","dateCreated":"2020-01-28T15:00:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:355"},{"text":"%md \n**REQUETE 3**\nPour une source de données passée en paramètre (gkg.SourceCommonName) affichez les thèmes, personnes, lieux dont les articles de cette sources parlent ainsi que le le nombre d’articles et le ton moyen des articles (pour chaque thème/personne/lieu); permettez une agrégation par jour/mois/année.","dateUpdated":"2020-01-28T15:00:40+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p><strong>REQUETE 3</strong><br/>Pour une source de données passée en paramètre (gkg.SourceCommonName) affichez les thèmes, personnes, lieux dont les articles de cette sources parlent ainsi que le le nombre d’articles et le ton moyen des articles (pour chaque thème/personne/lieu); permettez une agrégation par jour/mois/année.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1580223640663_-1056306964","id":"20200122-211709_1592504177","dateCreated":"2020-01-28T15:00:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:356"},{"text":"val gkg_english_DF = gkg_english_RDD.map(_.split(\"\\t\")).toDF()\nval gkg_other_DF = gkg_other_RDD.map(_.split(\"\\t\")).toDF()","dateUpdated":"2020-01-28T15:00:40+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"gkg_english_DF: org.apache.spark.sql.DataFrame = [value: array<string>]\ngkg_other_DF: org.apache.spark.sql.DataFrame = [value: array<string>]\n"}]},"apps":[],"jobName":"paragraph_1580223640664_-1058230708","id":"20200122-211745_2059739665","dateCreated":"2020-01-28T15:00:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:357"},{"text":"val gkg_DF = gkg_english_DF.union(gkg_other_DF)","dateUpdated":"2020-01-28T15:00:40+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"gkg_DF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [value: array<string>]\n"}]},"apps":[],"jobName":"paragraph_1580223640664_-1058230708","id":"20200123-143644_2024569871","dateCreated":"2020-01-28T15:00:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:358"},{"text":"// Sélection des colonnes intéressantes pour la requête \nval gkg_conv = gkg_DF.withColumn(\"_tmp\",$\"value\").select(\n    $\"_tmp\".getItem(0).as(\"record_id\"),\n    $\"_tmp\".getItem(1).as(\"date_timestamp\"),\n//    $\"_tmp\".getItem(2).as(\"SourceCollectionIdentifier\"),\n    $\"_tmp\".getItem(3).as(\"src_name\"),\n//    $\"_tmp\".getItem(4).as(\"DocumentIdentifier\"),\n//    $\"_tmp\".getItem(5).as(\"Counts\"),\n//    $\"_tmp\".getItem(6).as(\"V2Counts\"),\n    $\"_tmp\".getItem(7).as(\"themes\"),\n//    $\"_tmp\".getItem(8).as(\"V2Themes\"),\n    $\"_tmp\".getItem(9).as(\"locations\"),\n//    $\"_tmp\".getItem(10).as(\"V2Locations\"),\n    $\"_tmp\".getItem(11).as(\"persons\"),\n//    $\"_tmp\".getItem(12).as(\"V2Persons\"),\n//    $\"_tmp\".getItem(13).as(\"Organizations\"),\n//    $\"_tmp\".getItem(14).as(\"V2Organizations\"),\n    $\"_tmp\".getItem(15).as(\"tone\")\n//    $\"_tmp\".getItem(16).as(\"Dates\"),\n//    $\"_tmp\".getItem(17).as(\"GCAM\"),\n//    $\"_tmp\".getItem(18).as(\"SharingImage\"),\n//    $\"_tmp\".getItem(19).as(\"RelatedImages\"),\n//    $\"_tmp\".getItem(20).as(\"SocialImageEmbeds\"),\n//    $\"_tmp\".getItem(21).as(\"SocialVideoEmbeds\"),\n//    $\"_tmp\".getItem(22).as(\"Quotations\"),\n//    $\"_tmp\".getItem(23).as(\"AllNames\"),\n//    $\"_tmp\".getItem(24).as(\"Amounts\"),\n//    $\"_tmp\".getItem(25).as(\"TranslationInfo\"),\n//    $\"_tmp\".getItem(26).as(\"Extras\")\n    )","dateUpdated":"2020-01-28T15:00:40+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"gkg_conv: org.apache.spark.sql.DataFrame = [record_id: string, date_timestamp: string ... 5 more fields]\n"}]},"apps":[],"jobName":"paragraph_1580223640664_-1058230708","id":"20200122-211819_399176888","dateCreated":"2020-01-28T15:00:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:359"},{"text":"//Retraitement du format des colonnes\nval gkg_conv_2 = gkg_conv.withColumn(\"avg_tone\",substring_index($\"tone\",\",\",1))\n                         //.withColumn(\"year\",substring($\"date_timestamp\",0,4).cast(\"Int\"))\n                         //.withColumn(\"month-year\",substring($\"date_timestamp\",0,6).cast(\"Int\"))\n                         .filter(!($\"src_name\" === \"\"))\n                         .withColumn(\"date\",substring($\"date_timestamp\",0,8).cast(\"Int\"))\n                         .withColumn(\"persons_arr\",split(gkg_conv(\"persons\"),\";\"))\n                         .withColumn(\"locations_arr\",split(gkg_conv(\"locations\"),\";\"))\n                         .withColumn(\"themes_arr\",split(gkg_conv(\"themes\"),\";\"))","dateUpdated":"2020-01-28T15:00:40+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"gkg_conv_2: org.apache.spark.sql.DataFrame = [record_id: string, date_timestamp: string ... 10 more fields]\n"}]},"apps":[],"jobName":"paragraph_1580223640665_-1058615457","id":"20200122-211836_220784848","dateCreated":"2020-01-28T15:00:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:360"},{"text":"val gkg_conv_persons = gkg_conv_2.select($\"date\",$\"record_id\",$\"src_name\",$\"avg_tone\",explode_outer($\"persons_arr\").as(\"persons_2\"))","dateUpdated":"2020-01-28T15:00:40+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"gkg_conv_persons: org.apache.spark.sql.DataFrame = [date: int, record_id: string ... 3 more fields]\n"}]},"apps":[],"jobName":"paragraph_1580223640665_-1058615457","id":"20200122-211854_151274329","dateCreated":"2020-01-28T15:00:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:361"},{"text":"val gkg_conv_locations = gkg_conv_2.select($\"date\",$\"record_id\",$\"src_name\",$\"avg_tone\",explode_outer($\"locations_arr\").as(\"locations_2\"))","dateUpdated":"2020-01-28T15:00:40+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"gkg_conv_locations: org.apache.spark.sql.DataFrame = [date: int, record_id: string ... 3 more fields]\n"}]},"apps":[],"jobName":"paragraph_1580223640665_-1058615457","id":"20200122-211916_1354532690","dateCreated":"2020-01-28T15:00:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:362"},{"text":"val gkg_conv_themes = gkg_conv_2.select($\"date\",$\"record_id\",$\"src_name\",$\"avg_tone\",explode_outer($\"themes_arr\").as(\"themes_2\"))","dateUpdated":"2020-01-28T15:00:40+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"gkg_conv_themes: org.apache.spark.sql.DataFrame = [date: int, record_id: string ... 3 more fields]\n"}]},"apps":[],"jobName":"paragraph_1580223640666_-1057461211","id":"20200123-080159_1838152972","dateCreated":"2020-01-28T15:00:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:363"},{"text":"val requete3_persons_df = gkg_conv_persons.groupBy(\"date\",\"src_name\",\"persons_2\").agg(count($\"record_id\").cast(\"int\").as(\"nombre_articles\"),avg($\"avg_tone\").as(\"ton_moyen\"))\nval requete3_locations_df = gkg_conv_locations.groupBy(\"date\",\"src_name\",\"locations_2\").agg(count($\"record_id\").cast(\"int\").as(\"nombre_articles\"),avg($\"avg_tone\").as(\"ton_moyen\"))\nval requete3_themes_df = gkg_conv_themes.groupBy(\"date\",\"src_name\",\"themes_2\").agg(count($\"record_id\").cast(\"int\").as(\"nombre_articles\"),avg($\"avg_tone\").as(\"ton_moyen\"))","dateUpdated":"2020-01-28T15:00:40+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"requete3_persons_df: org.apache.spark.sql.DataFrame = [date: int, src_name: string ... 3 more fields]\nrequete3_locations_df: org.apache.spark.sql.DataFrame = [date: int, src_name: string ... 3 more fields]\nrequete3_themes_df: org.apache.spark.sql.DataFrame = [date: int, src_name: string ... 3 more fields]\n"}]},"apps":[],"jobName":"paragraph_1580223640666_-1057461211","id":"20200123-080212_1686080492","dateCreated":"2020-01-28T15:00:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:364"},{"text":"%md \n**REQUETE 4**\nDresser la cartographie des relations entre les pays d’après le ton des articles : pour chaque paire (pays1, pays2), calculer le nombre d’article, le ton moyen (aggrégations sur Année/Mois/Jour, filtrage par pays ou carré de coordonnées)","dateUpdated":"2020-01-28T15:00:40+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p><strong>REQUETE 4</strong><br/>Dresser la cartographie des relations entre les pays d’après le ton des articles : pour chaque paire (pays1, pays2), calculer le nombre d’article, le ton moyen (aggrégations sur Année/Mois/Jour, filtrage par pays ou carré de coordonnées)</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1580223640667_-1057845959","id":"20200122-211937_754452038","dateCreated":"2020-01-28T15:00:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:365"},{"text":"// On récupère les champs de la table mentions qui nous intéressent pour cette requête\nval temp1 = mentions_DF.withColumn(\"_tmp\", $\"value\").select(\n    $\"_tmp\".getItem(0).as(\"eventid\"),\n    $\"_tmp\".getItem(5).as(\"article\")\n    )","dateUpdated":"2020-01-28T15:00:40+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"temp1: org.apache.spark.sql.DataFrame = [eventid: string, article: string]\n"}]},"apps":[],"jobName":"paragraph_1580223640667_-1057845959","id":"20200122-211946_327469441","dateCreated":"2020-01-28T15:00:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:366"},{"text":"// aggrégation pour obtenir le nombre d'articles par event id \nval temp2 = temp1.groupBy(\"eventid\").agg(countDistinct($\"article\").as(\"nbarticles\"))","dateUpdated":"2020-01-28T15:00:40+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"temp2: org.apache.spark.sql.DataFrame = [eventid: string, nbarticles: bigint]\n"}]},"apps":[],"jobName":"paragraph_1580223640667_-1057845959","id":"20200122-212003_288892631","dateCreated":"2020-01-28T15:00:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:367"},{"text":"// On récupère les champs de la table events qui nous intéressent pour cette requête\n// Note: malgré le fait que la requête demande une possible localisation par les variables Actor1Geo_Lat et Actor1Geo_Long, je les ignore car elles ne ne nous disent rien des relations entre pays!\n// elles sont seulement liées au lieu où un évènement se produit, qui presque systématiquement ne correspond pas aux pays impliqués dans l'évènement (ex: Trump et Poutine se rencontrent au G20 de Hambourg zn 2017: localisation des 2 acteurs: GER, alors que pays de l'évènement: USA et RUS)\nval temp3 = events_DF.withColumn(\"_tmp\", $\"value\").select(\n    $\"_tmp\".getItem(0).as(\"eventid\"),\n    $\"_tmp\".getItem(1).as(\"day\"),\n    $\"_tmp\".getItem(2).as(\"month\"),\n    $\"_tmp\".getItem(3).as(\"year\"),\n    $\"_tmp\".getItem(7).as(\"actor1country\"),\n    $\"_tmp\".getItem(17).as(\"actor2country\"),\n    // $\"_tmp\".getItem(37).as(\"Actor1Geo_CountryCode\"),\n    // $\"_tmp\".getItem(45).as(\"Actor2Geo_CountryCode\"),\n    $\"_tmp\".getItem(34).as(\"avgtone\")\n    // $\"_tmp\".getItem(40).as(\"Actor1Geo_Lat\"),\n    // $\"_tmp\".getItem(41).as(\"Actor1Geo_Long\"),\n    // $\"_tmp\".getItem(48).as(\"Actor2Geo_Lat\"),\n    // $\"_tmp\".getItem(49).as(\"Actor2Geo_Long\")\n    )","dateUpdated":"2020-01-28T15:00:40+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"temp3: org.apache.spark.sql.DataFrame = [eventid: string, day: string ... 5 more fields]\n"}]},"apps":[],"jobName":"paragraph_1580223640668_-1059769704","id":"20200122-212022_1759580442","dateCreated":"2020-01-28T15:00:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:368"},{"text":"// On filtre pour enlever toutes les lignes ou Actor1CountryCode ou Actor2CountryCode sont blancs (énorme perte d'info, mais les lignes en question sont inutilisables pour la requête)\nval temp4 = temp3.filter(temp3(\"actor1country\") !== \"\").filter(temp3(\"actor2country\") !== \"\")","dateUpdated":"2020-01-28T15:00:40+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"warning: there were two deprecation warnings; re-run with -deprecation for details\ntemp4: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [eventid: string, day: string ... 5 more fields]\n"}]},"apps":[],"jobName":"paragraph_1580223640668_-1059769704","id":"20200122-212048_878111720","dateCreated":"2020-01-28T15:00:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:369"},{"text":"// On a un potentiel problème de swap: la possibilité que deux lignes parlent des mêmes pays, mais avec une paire inversée (ex: GER et FRA pour une ligne, FRA et GER pour l'autre)\n// Si on laisse en l'état, la requête reconnaîtra deux paires différentes, alors qu'il s'agit du même couple de pays, on doit donc harmoniser, et pour ça on choisit de mettre toujours la paire par ordre alphabétique\n// On crée un sous-dataframe qui contient les paires déjà en ordre alphabétique:\nval temp5 = temp4.filter(temp3(\"actor1country\") <= temp3(\"actor2country\"))\n// On crée un autre sous-dataframe qui contient les paires en ordre anti-alphabétique, puis on inverse les entrées des colonnes Actor1CountryCode et Actor2CountryCode\nval temp6 = temp4.filter(temp3(\"actor1country\") > temp3(\"actor2country\")).withColumnRenamed(\"actor1country\",\"temp\").withColumnRenamed(\"actor2country\",\"actor1country\").withColumnRenamed(\"temp\",\"actor2country\").select(\"eventid\", \"day\", \"month\", \"year\", \"actor1country\", \"actor2country\", \"avgtone\")\n// on fait une union des deux sets pour avoir un dataframe entièrement classé\nval temp7 = temp5.union(temp6)","dateUpdated":"2020-01-28T15:00:40+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"temp5: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [eventid: string, day: string ... 5 more fields]\ntemp6: org.apache.spark.sql.DataFrame = [eventid: string, day: string ... 5 more fields]\ntemp7: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [eventid: string, day: string ... 5 more fields]\n"}]},"apps":[],"jobName":"paragraph_1580223640668_-1059769704","id":"20200122-212103_1686172976","dateCreated":"2020-01-28T15:00:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:370"},{"text":"// on fait une jointure des dataframes temp 2 et temp7 sur l'attribut GlobalEventID\nval temp8 = temp2.join(temp7, \"eventid\")","dateUpdated":"2020-01-28T15:00:40+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"temp8: org.apache.spark.sql.DataFrame = [eventid: string, nbarticles: bigint ... 6 more fields]\n"}]},"apps":[],"jobName":"paragraph_1580223640669_-1060154453","id":"20200122-212126_689553290","dateCreated":"2020-01-28T15:00:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:371"},{"text":"// On réorganise: on drop la colonne eventid qui ne sert plus à rien (seule la paire de pays nous intéresse), et on réorganise pour avoir la paire de pays en clé\nval temp9 = temp8.drop(\"eventid\").select(\"actor1country\", \"actor2country\", \"avgtone\", \"nbarticles\", \"day\", \"month\", \"year\")","dateUpdated":"2020-01-28T15:00:40+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"temp9: org.apache.spark.sql.DataFrame = [actor1country: string, actor2country: string ... 5 more fields]\n"}]},"apps":[],"jobName":"paragraph_1580223640669_-1060154453","id":"20200122-212138_935206151","dateCreated":"2020-01-28T15:00:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:372"},{"text":"val requete4_df = temp9.groupBy(\"actor1country\",\"actor2country\", \"day\", \"month\", \"year\").agg(count($\"nbarticles\"),avg($\"avgtone\")).withColumnRenamed(\"count(nbarticles)\",\"nbarticles\").withColumnRenamed(\"avg(avgtone)\",\"avgtone\")","dateUpdated":"2020-01-28T15:00:40+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"requete4_df: org.apache.spark.sql.DataFrame = [actor1country: string, actor2country: string ... 5 more fields]\n"}]},"apps":[],"jobName":"paragraph_1580223640670_-1059000206","id":"20200123-091823_61719800","dateCreated":"2020-01-28T15:00:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:373"},{"text":"%md\n#### Etape 3 : Stockage des tables dans Cassandra\nDans cette partie, nous allons stocké les tables créées précédemment dans Cassandra afin de pouvoir excéuter des requêtes dessus dans le but de répondre aux différentes problématiques.","dateUpdated":"2020-01-28T15:00:40+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Etape 3 : Stockage des tables dans Cassandra</h4>\n<p>Dans cette partie, nous allons stocké les tables créées précédemment dans Cassandra afin de pouvoir excéuter des requêtes dessus dans le but de répondre aux différentes problématiques.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1580223640670_-1059000206","id":"20200122-212225_293256288","dateCreated":"2020-01-28T15:00:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:374"},{"text":"// TABLE REQUETE 1\n// On choisit un facteur de réplication de 3 sur nos noeuds\n// Le Replication Factor doit forcément être inférieur ou égal au nombre de nœuds du cluster.\n\nCassandraConnector(sc.getConf).withSessionDo { session =>\n      session.execute(\n          \"\"\"\n             DROP KEYSPACE gdelt;\n          \"\"\"\n          )\n      session.execute(\n          \"\"\"\n             CREATE KEYSPACE IF NOT EXISTS gdelt\n             WITH REPLICATION = {'class': 'SimpleStrategy', 'replication_factor': 3 };\n          \"\"\")\n          session.execute(\n          \"\"\"\n             CREATE TABLE IF NOT EXISTS gdelt.requete1(\n                globaleventid int,\n                day_mention int,\n                language text,\n                day_event text,\n                nombre_articles int,\n                actioncountry text,\n             PRIMARY KEY ((language, actioncountry), day_mention));\n          \"\"\"\n        )\n}\n\nrequete1_df.write\n      .cassandraFormat(\"requete1\", \"gdelt\")\n      .mode(org.apache.spark.sql.SaveMode.Overwrite)\n      .options(Map(\"confirm.truncate\" -> \"true\"))\n      .save()\n      \nval req1 = spark.read\n      .cassandraFormat(\"requete1\", \"gdelt\")\n      .load()\n      \nreq1.createOrReplaceTempView(\"vue_req1\")\n//req1.orderBy(desc(\"actioncountry\")).show()","dateUpdated":"2020-01-28T15:13:37+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"req1: org.apache.spark.sql.DataFrame = [globaleventid: int, day_mention: int ... 4 more fields]\n"}]},"apps":[],"jobName":"paragraph_1580223640671_-1059384955","id":"20200122-212241_1773429187","dateCreated":"2020-01-28T15:00:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:375"},{"text":"// TABLE REQUETE 2\n\nCassandraConnector(sc.getConf).withSessionDo { session =>\n          session.execute (\n          \"\"\"\n             CREATE TABLE IF NOT EXISTS gdelt.requete2(\n                day_event int,\n                actioncountry text,\n                globaleventid int,\n                nombre_mentions int,\n             PRIMARY KEY (actioncountry, day_event, nombre_mentions));\n          \"\"\"\n        )\n}\n\nrequete2_df.write\n      .cassandraFormat(\"requete2\", \"gdelt\")\n      .mode(org.apache.spark.sql.SaveMode.Overwrite)\n      .options(Map(\"confirm.truncate\" -> \"true\"))\n      .save()\n      \nval req2 = spark.read\n      .cassandraFormat(\"requete2\", \"gdelt\")\n      .load()\n      \nreq2.createOrReplaceTempView(\"vue_req2\")","dateUpdated":"2020-01-28T15:00:40+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"req2: org.apache.spark.sql.DataFrame = [day_event: int, actioncountry: string ... 2 more fields]\n"}]},"apps":[],"jobName":"paragraph_1580223640671_-1059384955","id":"20200122-212258_420643896","dateCreated":"2020-01-28T15:00:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:376"},{"text":"// TABLE REQUETE 3 PERSONS\nCassandraConnector(sc.getConf).withSessionDo { session =>\n          session.execute (\n          \"\"\"\n             CREATE TABLE IF NOT EXISTS gdelt.requete3_persons(\n                date int,\n                src_name text,\n                persons_2 text,\n                nombre_articles int,\n                ton_moyen double,\n             PRIMARY KEY (src_name, date));\n          \"\"\"\n        )\n}\n\nrequete3_persons_df.write\n      .cassandraFormat(\"requete3_persons\", \"gdelt\")\n      .mode(org.apache.spark.sql.SaveMode.Overwrite)\n      .options(Map(\"confirm.truncate\" -> \"true\"))\n      .save()\n      \nval req3_persons = spark.read\n      .cassandraFormat(\"requete3_persons\", \"gdelt\")\n      .load()\n      \nreq3_persons.createOrReplaceTempView(\"vue_req3_persons\")","dateUpdated":"2020-01-28T15:03:48+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1580223640671_-1059384955","id":"20200122-212323_475143603","dateCreated":"2020-01-28T15:00:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:377"},{"text":"// TABLE REQUETE 3 LOCATIONS\nCassandraConnector(sc.getConf).withSessionDo { session =>\n          session.execute (\n          \"\"\"\n             CREATE TABLE IF NOT EXISTS gdelt.requete3_locations(\n                date int,\n                src_name text,\n                locations_2 text,\n                nombre_articles int,\n                ton_moyen double,\n             PRIMARY KEY (src_name, date));\n          \"\"\"\n        )\n}\n\nrequete3_locations_df.write\n      .cassandraFormat(\"requete3_locations\", \"gdelt\")\n      .mode(org.apache.spark.sql.SaveMode.Overwrite)\n      .options(Map(\"confirm.truncate\" -> \"true\"))\n      .save()\n      \nval req3_locations = spark.read\n      .cassandraFormat(\"requete3_locations\", \"gdelt\")\n      .load()\n      \nreq3_locations.createOrReplaceTempView(\"vue_req3_locations\")","dateUpdated":"2020-01-28T15:03:37+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"req3_locations: org.apache.spark.sql.DataFrame = [src_name: string, date: int ... 3 more fields]\n"}]},"apps":[],"jobName":"paragraph_1580223640672_-1073620664","id":"20200123-153130_1091536340","dateCreated":"2020-01-28T15:00:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:378"},{"text":"// TABLE REQUETE 3 THEMES\nCassandraConnector(sc.getConf).withSessionDo { session =>\n          session.execute(\n              \"\"\"\n              DROP TABLE gdelt.requete3_themes\n              \"\"\"\n              )\n          session.execute (\n          \"\"\"\n             CREATE TABLE IF NOT EXISTS gdelt.requete3_themes(\n                date int,\n                src_name text,\n                themes_2 text,\n                nombre_articles int,\n                ton_moyen double,\n             PRIMARY KEY (src_name, date));\n          \"\"\"\n        )\n}\n\nrequete3_themes_df.write\n      .cassandraFormat(\"requete3_themes\", \"gdelt\")\n      .mode(org.apache.spark.sql.SaveMode.Overwrite)\n      .options(Map(\"confirm.truncate\" -> \"true\"))\n      .save()\n      \nval req3_themes = spark.read\n      .cassandraFormat(\"requete3_themes\", \"gdelt\")\n      .load()\n      \nreq3_themes.createOrReplaceTempView(\"vue_req3_themes\")","dateUpdated":"2020-01-28T15:00:40+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"req3_themes: org.apache.spark.sql.DataFrame = [src_name: string, date: int ... 3 more fields]\n"}]},"apps":[],"jobName":"paragraph_1580223640672_-1073620664","id":"20200123-153351_900455031","dateCreated":"2020-01-28T15:00:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:379"},{"text":"// TABLE REQUETE 4\nCassandraConnector(sc.getConf).withSessionDo { session =>\n          session.execute (\n          \"\"\"\n             CREATE TABLE IF NOT EXISTS gdelt.requete4(\n                actor1country text,\n                actor2country text,\n                day text,\n                month text,\n                year text,\n                nbarticles int,\n                avgtone double,\n             PRIMARY KEY ((actor1country, actor2country), day, month, year));\n          \"\"\"\n        )\n}\n\nrequete4_df.write\n      .cassandraFormat(\"requete4\", \"gdelt\")\n      .mode(org.apache.spark.sql.SaveMode.Overwrite)\n      .options(Map(\"confirm.truncate\" -> \"true\"))\n      .save()\n      \nval req4 = spark.read\n      .cassandraFormat(\"requete4\", \"gdelt\")\n      .load()\n      \nreq4.createOrReplaceTempView(\"vue_req4\")","dateUpdated":"2020-01-28T15:00:40+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"req4: org.apache.spark.sql.DataFrame = [actor1country: string, actor2country: string ... 5 more fields]\n"}]},"apps":[],"jobName":"paragraph_1580223640672_-1073620664","id":"20200122-212711_1099986809","dateCreated":"2020-01-28T15:00:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:380"}],"name":"Preparation","id":"2EZZAMEV9","angularObjects":{"2F15CMNF6:shared_process":[],"2EY4UVC56:shared_process":[],"2EY5N461P:shared_process":[],"2EXV27Y7X:shared_process":[],"2EYJZK9AX:shared_process":[],"2EZHY7UPH:shared_process":[],"2F1C6SQXY:shared_process":[],"2F231W94Q:shared_process":[],"2F1G3AYTH:shared_process":[],"2EZY71887:shared_process":[],"2F2Q93SK2:shared_process":[],"2EZFNJYQ8:shared_process":[],"2EYR39NYZ:shared_process":[],"2EYJ2CYFC:shared_process":[],"2EY2FTBJJ:shared_process":[],"2EZTTKH2J:shared_process":[],"2EZS2R3SJ:shared_process":[],"2EYX4SARM:shared_process":[],"2F1DS58DM:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}
