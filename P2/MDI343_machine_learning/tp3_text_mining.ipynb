{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP : Analyse de sentiments dans les critiques de films"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectifs\n",
    "\n",
    "1. Implémenter une manière simple de représenter des données textuelles\n",
    "2. Implémenter un modèle d'apprentissage statistique basique\n",
    "3. Utiliser ces représentations et ce modèle pour une tâche d'analyse de sentiments\n",
    "4. Tenter d'améliorer les résultats avec des outils venus du traitement automatique du langage\n",
    "5. Comparer les résultats avec une l'implémentation de Scikit-Learn, et avec d'autres méthodes de représentation ou d'apprentissage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dépendances nécessaires\n",
    "\n",
    "Pour les objectifs 4. et 5., on aura besoin des packages suivants:\n",
    "- The Machine Learning API Scikit-learn : http://scikit-learn.org/stable/install.html\n",
    "- The Natural Language Toolkit : http://www.nltk.org/install.html\n",
    "\n",
    "Les deux sont disponibles avec Anaconda: https://anaconda.org/anaconda/nltk et https://anaconda.org/anaconda/scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as op\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charger les données\n",
    "\n",
    "L'ensemble des données est disponible ici: https://ai.stanford.edu/~amaas/data/sentiment/\n",
    "(pour faciliter la récupération, vous pouvez simplement décompresser [cette archive](https://drive.google.com/file/d/1t_cai2X5VUt1yG2DHiMDBCfpRuz562wn/view?usp=sharing) dans le dossier du notebook)\n",
    "\n",
    "On récupère les données textuelles dans la variable *texts*\n",
    "\n",
    "On récupère les labels dans la variable $y$ qui en contient *len(texts)* : $0$ indique que la critique correspondante est négative tandis que $1$ qu'elle est positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "filenames_neg = sorted(glob(op.join('.', 'data', 'imdb1', 'neg', '*.txt')))\n",
    "filenames_pos = sorted(glob(op.join('.', 'data', 'imdb1', 'pos', '*.txt')))\n",
    "\n",
    "texts_neg = [open(f, encoding=\"utf8\").read() for f in filenames_neg]\n",
    "texts_pos = [open(f, encoding=\"utf8\").read() for f in filenames_pos]\n",
    "texts = texts_neg + texts_pos\n",
    "\n",
    "#Return an array of [1,len(texts)], filled with ones. \n",
    "y = np.ones(len(texts), dtype=np.int)\n",
    "y[:len(texts_neg)] = 0.\n",
    "\n",
    "print(\"%d documents\" % len(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idée principale\n",
    "\n",
    "On dispose d'une critique étant en fait une liste de mots $s = (w_1, ..., w_N)$, et l'on cherche à trouver la classe associée $c$ - qui dans notre cas, peut-être $c = 0$ ou $c = 1$. L'objectif est donc de trouver pour chaque critique $s$ la classe $\\hat{c}$ maximisant la probabilité conditionelle **$P(c|s)$** : \n",
    "\n",
    "$$\\hat{c} = \\underset{c}{\\mathrm{argmax}}\\, P(c|s) = \\underset{c}{\\mathrm{argmax}}\\,\\frac{P(s|c)P(c)}{P(s)}$$\n",
    "\n",
    "**Hypothèse : P(s) est constante pour chaque classe** :\n",
    "\n",
    "$$\\hat{c} = \\underset{c}{\\mathrm{argmax}}\\,\\frac{P(s|c)P(c)}{P(s)} = \\underset{c}{\\mathrm{argmax}}\\,P(s|c)P(c)$$\n",
    "\n",
    "**Hypothèse naïve : les différentes variables (mots) d'une critique sont indépendantes entre elles** : \n",
    "\n",
    "$$P(s|c) = P(w_1, ..., w_N|c)=\\Pi_{i=1..N}P(w_i|c)$$\n",
    "\n",
    "On va donc pouvoir se servir des critiques annotées à notre disposition pour **estimer les probabilités $P(w|c)$ pour chaque mot $w$ étant donné les deux classes $c$**. Ces critiques vont nous permettre d'apprendre à évaluer la \"compatibilité\" entre les mots et classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vue générale\n",
    "\n",
    "### Entraînement: Estimer les probabilités\n",
    "\n",
    "Pour chaque mot $w$ du vocabulaire $V$, $P(w|c)$ est le nombre d'occurences de $w$ dans une critique ayant pour classe $c$, divisé par le nombre total d'occurences dans $c$. Si on note $T(w,c)$ ce nombre d'occurences, on obtient:\n",
    "\n",
    "$$P(w|c) = \\text{Fréquence de }w\\text{ dans }c = \\frac{T(w,c)}{\\sum_{w' \\in V} T(w', c)}$$\n",
    "\n",
    "### Test: Calcul des scores\n",
    "\n",
    "Pour faciliter les calculs et éviter les erreurs d'*underflow* et d'approximation, on utilise le \"log-sum trick\", et on passe l'équation en log-probabilités : \n",
    "\n",
    "$$\\hat{c} =  \\underset{c}{\\mathrm{argmax}}\\, P(c|s) = \\underset{c}{\\mathrm{argmax}}\\, \\left[ \\mathrm{log}(P(c)) + \\sum_{i=1..N}log(P(w_i|c)) \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Représentation adaptée des documents\n",
    "\n",
    "Notre modèle statistique, comme la plupart des modèles appliqués aux données textuelles, utilise les comptes d'occurences de mots dans un document. Ainsi, une manière très pratique de représenter un document est d'utiliser un vecteur \"Bag-of-Words\" (BoW), contenant les comptes de chaque mot (indifférement de leur ordre d'apparition) dans le document. \n",
    "\n",
    "Si on considère l'ensemble de tous les mots apparaissant dans nos $T$ documents d'apprentissage, que l'on note $V$ (Vocabulaire), on peut créer **un index**, qui est une bijection associant à chaque mot $w$ un entier, qui sera sa position dans $V$. \n",
    "\n",
    "Ainsi, pour un document extrait d'un ensemble de documents contenant $|V|$ mots différents, une représentation BoW sera un vecteur de taille $|V|$, dont la valeur à l'indice d'un mot $w$ sera son nombre d'occurences dans le document. \n",
    "\n",
    "On peut utiliser la classe **CountVectorizer** de scikit-learn pour mieux comprendre:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['I walked down down the boulevard', 'I walked down the avenue',\n",
    "          'I ran down the boulevard', 'I walk down the city','I walk down the the avenue']\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "Bow = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(vectorizer.get_feature_names())\n",
    "Bow.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On affiche d'abord la liste contenant les mots ordonnés selon leur indice (On note que les mots de 2 caractères ou moins ne sont pas pris en compte)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Détail: entraînement\n",
    "\n",
    "L'idée est d'extraire le nombre d'occurences $T(w,c)$ de chaque mot $w$ pour chaque classe $c$, ce qui permettra de calculer la matrice de probabilités conditionelles $\\pmb{P}$ telle que: $$\\pmb{P}_{w,c} = P(w|c)$$\n",
    "\n",
    "Notons que le nombre d'occurences $T(w,c)$ peut être obtenu facilement à partir des représentations BoW de l'ensemble des documents.\n",
    "\n",
    "### Procédure:\n",
    "<img src=\"algo_train.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "## Détail: test\n",
    "\n",
    "Nous connaissons maintenant les probabilités conditionelles données par la matrice $\\pmb{P}$. \n",
    "Il faut maintenant obtenir $P(s|c)$ pour le document courant. Cette quantité s'obtient à l'aide d'un calcul simple impliquant la représentation BoW du document et $\\pmb{P}$.\n",
    "\n",
    "### Procédure:\n",
    "<img src=\"algo_test.png\" alt=\"Drawing\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing du texte: obtenir les représentations BoW\n",
    "\n",
    "Fonction **à compléter**. Elle renvoie la représentation BoW d'un document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quelques pointeurs pour les débutants en Python : \n",
    "\n",
    "- ```string_1.split(string_2)``` : split the ```string_1``` variable using the ```string_2``` pattern\n",
    "\n",
    "- ```my_list.append(value)``` : put the variable ```value``` at the end of the list ```my_list```\n",
    "\n",
    "-  ```words = set()``` : create a set, which is a list of unique values\n",
    "\n",
    "- ```words.union(my_list)``` : extend the set ```words```\n",
    "\n",
    "- ```dict(zip(keys, values)))``` : create a dictionnary\n",
    "\n",
    "- ```for k, text in enumerate(texts)``` : syntax for a loop with the index, ```texts``` begin a list (of texts !)\n",
    "\n",
    "- ```len(my-list)``` : length of the list ```my_list```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(texts):\n",
    "    \"\"\"Vectorize text : return count of each word in the text snippets\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    texts : list of str\n",
    "        The texts\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    vocabulary : dict\n",
    "        A dictionary that points to an index in counts for each word.\n",
    "    counts : ndarray, shape (n_samples, n_features)\n",
    "        The counts of each word in each text.\n",
    "    \"\"\"\n",
    "    \n",
    "    words = set()\n",
    "    for text in texts:\n",
    "        pass\n",
    "    n_features = 10\n",
    "    counts = np.zeros((len(texts, n_features)))\n",
    "    return vocabulary, counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naïve Bayes \n",
    "\n",
    "Classe vide : fonctions **à compléter** \n",
    "```python\n",
    "def fit(self, X, y)\n",
    "``` \n",
    "**Entraînement** : va apprendre un modèle statistique basés sur les représentations $X$ correspondant aux labels $y$.\n",
    "\n",
    "```python\n",
    "def predict(self, X)\n",
    "```\n",
    "**Testing** : va renvoyer les labels prédits par le modèle pour les représentations $X$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quelques pointeurs pour les débutants en Python : \n",
    "\n",
    "Utiliser l'API Numpy pour travailler avec des tenseurs\n",
    "\n",
    "\n",
    "- ```X.shape``` : for a ```numpy.array```, return the dimension of the tensor\n",
    "\n",
    "- ```np.zeros((dim_1, dim_2,...))``` : create a tensor filled with zeros\n",
    "\n",
    "- ```np.sum(X, axis = n)``` : sum the tensor over the axis n\n",
    "\n",
    "- ```np.mean(X, axis = n)```\n",
    "\n",
    "- ```np.argmax(X, axis = n)```\n",
    "\n",
    "- ```np.log(X)```\n",
    "\n",
    "- ```np.dot(X_1, X_1)``` : Matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NB(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return (np.random.randn(len(X)) > 0).astype(np.int)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return np.mean(self.predict(X) == y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expérimentation\n",
    "\n",
    "On utilise la moitié des données pour l'entraînement, l'autre pour tester le modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ici, on part d'un cinquième des données, pour des questions de temps de calcul\n",
    "texts_red = texts[0::5]\n",
    "y_red = y [0::5]\n",
    "\n",
    "print('Nombre de documents:', len(y_red))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc, X = count_words(texts_red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nb = NB()\n",
    "nb.fit(X[::2], y_red[::2])\n",
    "print(nb.score(X[1::2], y_red[1::2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation \n",
    "\n",
    "Avec la fonction *cross_val_score* de scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(nb, X, y_red, cv=5)\n",
    "print('Score de classification: %s (std %s)' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluer les performances: \n",
    "\n",
    "**Quelles sont les points forts et les points faibles de ce système ? Comment y remédier ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pour aller plus loin: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Améliorer les représentations\n",
    "\n",
    "On utilise la function \n",
    "```python\n",
    "CountVectorizer\n",
    "``` \n",
    "de scikit-learn pour constituer notre corpus. Elle nous permettra d'améliorer facilement nos représentations BoW.\n",
    "\n",
    "#### Tf-idf:\n",
    "\n",
    "Il s'agit du produit de la fréquence du terme (TF) et de sa fréquence inverse dans les documents (IDF).\n",
    "Cette méthode est habituellement utilisée pour extraire l'importance d'un terme $i$ dans un document $j$ relativement au reste du corpus, à partir d'une matrice d'occurences $mots \\times documents$. Ainsi, pour une matrice $\\mathbf{T}$ de $|V|$ termes et $D$ documents:\n",
    "$$\\text{TF}(T, w, d) = \\frac{T_{w,d}}{\\sum_{w'=1}^{|V|} T_{w',d}} $$\n",
    "\n",
    "$$\\text{IDF}(T, w) = \\log\\left(\\frac{D}{|\\{d : T_{w,d} > 0\\}|}\\right)$$\n",
    "\n",
    "$$\\text{TF-IDF}(T, w, d) = \\text{TF}(X, w, d) \\cdot \\text{IDF}(T, w)$$\n",
    "\n",
    "On peut l'adapter à notre cas en considérant que le contexte du deuxième mot est le document. Cependant, TF-IDF est généralement plus adaptée aux matrices peu denses, puisque cette mesure pénalisera les termes qui apparaissent dans une grande partie des documents. \n",
    "    \n",
    "#### Ne pas prendre en compte les mots trop fréquents:\n",
    "\n",
    "On peut utiliser l'option \n",
    "```python\n",
    "max_df=1.0\n",
    "```\n",
    "pour modifier la quantité de mots pris en compte. \n",
    "\n",
    "#### Essayer différentes granularités:\n",
    "\n",
    "Plutôt que de simplement compter les mots, on peut compter les séquences de mots - de taille limitée, bien sur. \n",
    "On appelle une séquence de $n$ mots un $n$-gram: essayons d'utiliser les 2 et 3-grams (bi- et trigrams).\n",
    "On peut aussi tenter d'utiliser les séquences de caractères à la place de séquences de mots.\n",
    "\n",
    "On s'intéressera aux options \n",
    "```python\n",
    "analyzer='word'\n",
    "```\n",
    "et \n",
    "```python\n",
    "ngram_range=(1, 2)\n",
    "```\n",
    "que l'on changera pour modifier la granularité. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## On peut définir un pipeline que l'on modifiera pour expérimenter.\n",
    "\n",
    "pipeline_base = Pipeline([\n",
    "    ('vect', CountVectorizer(analyzer='word', stop_words=None)),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "scores = cross_val_score(pipeline_base, texts_red, y_red, cv=5)\n",
    "print(\"Classification score: %s (std %s)\",(np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Language Toolkit (NLTK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming \n",
    "\n",
    "Permet de revenir à la racine d'un mot: on peut ainsi grouper différents mots autour de la même racine, ce qui facilite la généralisation. Utiliser:\n",
    "```python\n",
    "from nltk import SnowballStemmer\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemple d'utilisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['singers', 'cat', 'generalization', 'philosophy', 'psychology', 'philosopher']\n",
    "for word in words:\n",
    "    print('word : %s ; stemmed : %s' %(word, stemmer.stem(word)))#.decode('utf-8'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformation des données:\n",
    "\n",
    "Classe vide : function **à compléter** \n",
    "```python\n",
    "def stem(X)\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(X): \n",
    "    X_stem = []\n",
    "    for text in X:\n",
    "        pass\n",
    "    return X_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_stemmed = stem(texts_red)\n",
    "voc, X = count_words(texts_stemmed)\n",
    "nb = NB()\n",
    "\n",
    "scores = cross_val_score(nb, X, y_red, cv=5)\n",
    "print('Score de classification: %s (std %s)' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partie du discours\n",
    "\n",
    "Pour généraliser, on peut aussi utiliser les parties du discours (Part of Speech, POS) des mots, ce qui nous permettra  de filtrer l'information qui n'est potentiellement pas utile au modèle. On va récupérer les POS des mots à l'aide des fonctions:\n",
    "```python\n",
    "from nltk import pos_tag, word_tokenize\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemple d'utilisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pos_tag(word_tokenize(('I am Sam')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Détails des significations des tags POS: https://stackoverflow.com/questions/15388831/what-are-all-possible-pos-tags-of-nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Transformation des données:\n",
    "\n",
    "Classe vide : fonction **à compléter** \n",
    "```python\n",
    "def pos_tag_filter(X, good_tags=['NN', 'VB', 'ADJ', 'RB'])\n",
    "``` \n",
    "\n",
    "Ne garder que les noms, adverbes, verbes et adjectifs pour notre modèle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag_filter(X, good_tags=['NN', 'VB', 'ADJ', 'RB']):\n",
    "    X_pos = []\n",
    "    for text in X:\n",
    "        pass\n",
    "    return X_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_POS = pos_tag_filter(texts_red)\n",
    "voc, X = count_words(texts_POS)\n",
    "nb = NB()\n",
    "\n",
    "scores = cross_val_score(nb, X, y_red, cv=5)\n",
    "print('Score de classification: %s (std %s)' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop-words \n",
    "\n",
    "Les \"stop-words\" sont les mots apparaissant fréquemment dans les données et que l'on juge non représentatifs. On les considère comme du bruit. Une liste de stop-words est disponible dans le fichier *english.stop*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFile(fileName):\n",
    "    \"\"\"\n",
    "     * Code for reading a file.  you probably don't want to modify anything here, \n",
    "     * unless you don't like the way we segment files.\n",
    "    \"\"\"\n",
    "    contents = []\n",
    "    f = open(fileName)\n",
    "    for line in f:\n",
    "        contents.append(line)\n",
    "    f.close()\n",
    "    result = ('\\n'.join(contents)).split() \n",
    "    return result\n",
    "\n",
    "sw = readFile('english.stop')\n",
    "sw[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Transformation des données:\n",
    "\n",
    "Classe vide : fonction **à compléter** \n",
    "```python\n",
    "def filterStopWords(X)\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterStopWords(X):\n",
    "    \"\"\"Filters stop words.\"\"\"\n",
    "    X_filtered = []\n",
    "    for text in X:\n",
    "        pass\n",
    "    return X_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_stop = pos_tag_filter(texts_red)\n",
    "voc, X = count_words(texts_stop)\n",
    "nb = NB()\n",
    "\n",
    "scores = cross_val_score(nb, X, y_red, cv=5)\n",
    "print('Score de classification: %s (std %s)' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Utilisation d'un classifieur plus complexe ?\n",
    "\n",
    "On peut utiliser les implémentations scikit-learn de classifieurs moins naïfs, comme la régression logistique ou les SVM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
